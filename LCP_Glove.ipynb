{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LCP_Glove.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbzg2Ocj7EOzlXg4MwssoW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sass0987/Lexical-Complexity-Prediction-CMSC-516/blob/main/LCP_Glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoMP9ehkCncl",
        "outputId": "0d95b0a2-ccd5-4f68-a2f1-ffcdc25343f6"
      },
      "source": [
        "!pip install spacy-syllables\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip3 install wordfreq\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy-syllables\n",
            "  Downloading spacy_syllables-3.0.1-py3-none-any.whl (6.6 kB)\n",
            "Collecting spacy<4.0.0,>=3.0.3\n",
            "  Downloading spacy-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 34.0 MB/s \n",
            "\u001b[?25hCollecting pyphen<0.11.0,>=0.10.0\n",
            "  Downloading Pyphen-0.10.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 37.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.0.6)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.11.3)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 71.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (0.8.2)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.23.0)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (4.62.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.0.6)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 73.8 MB/s \n",
            "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0.0,>=3.0.3->spacy-syllables) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.0.3->spacy-syllables) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.3->spacy-syllables) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy, pyphen, spacy-syllables\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 pyphen-0.10.0 spacy-3.2.0 spacy-legacy-3.0.8 spacy-loggers-1.0.1 spacy-syllables-3.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 25.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting wordfreq\n",
            "  Downloading wordfreq-2.5.1.tar.gz (56.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 56.8 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (1.0.2)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (3.3.0)\n",
            "Collecting regex>=2020.04.04\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 38.0 MB/s \n",
            "\u001b[?25hCollecting ftfy>=3.0\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy>=3.0->wordfreq) (0.2.5)\n",
            "Building wheels for collected packages: wordfreq, ftfy\n",
            "  Building wheel for wordfreq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordfreq: filename=wordfreq-2.5.1-py3-none-any.whl size=56830991 sha256=936ba68fb26d7a6ae33689342f0b72030e17bc9ace54d3ed788b83a7bd35eaca\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/d4/2d/741daa49d00231b4c5afbec15e79f361d9e76961f1a5fc02e5\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=9b277d4f777dfabd2cb2bac6b1f9045941ebf5e7ec429c0f15a16dcd8828fa76\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built wordfreq ftfy\n",
            "Installing collected packages: regex, ftfy, wordfreq\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed ftfy-6.0.3 regex-2021.11.10 wordfreq-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C3aws_rCqzT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from wordfreq import word_frequency\n",
        "from scipy import stats\n",
        "import csv\n",
        "import spacy\n",
        "from spacy_syllables import SpacySyllables\n",
        "import random\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXmAW8UlCvgP",
        "outputId": "ff702301-cbd2-4a4f-cc5e-462b76478961"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-24 20:00:51--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-11-24 20:00:51--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-11-24 20:00:51--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.09MB/s    in 2m 40s  \n",
            "\n",
            "2021-11-24 20:03:32 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ36iTLJC2p7",
        "outputId": "4c9d2ae4-7c66-4512-a720-efdc601f3672"
      },
      "source": [
        "!unzip glove*.zip\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABj2uVEeC7be"
      },
      "source": [
        "# https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch\n",
        "# Seed all rngs for deterministic results\n",
        "def seed_all(seed = 0):\n",
        "  random.seed(0)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWgggTvRC8Cq"
      },
      "source": [
        "seed_all(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU0HGXZhC8FP"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY2pToatC8Ie",
        "outputId": "e9db46f6-bc06-48bf-e125-e2ab411b63c0"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"syllables\", after='tagger') # Add the syllable tagger pipe\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy_syllables.SpacySyllables at 0x7f1e7e3283d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4DYvE9oC8Jj"
      },
      "source": [
        "SINGLE_TRAIN_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/train/lcp_single_train.tsv\"\n",
        "SINGLE_TEST_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/test-labels/lcp_single_test.tsv\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvTCL2NPDJ8U"
      },
      "source": [
        "def get_data_frames():\n",
        "  df_train_single = pd.read_csv(SINGLE_TRAIN_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "  df_test_single = pd.read_csv(SINGLE_TEST_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "\n",
        "  \n",
        "  return df_train_single, df_test_single"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WJ_KZLFDK0e"
      },
      "source": [
        "df_train_single, df_test_single = get_data_frames()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8cz-gPBDK3I"
      },
      "source": [
        "single_tokens_train_raw = df_train_single[\"token\"].astype(str).to_list()\n",
        "single_tokens_test_raw = df_test_single[\"token\"].astype(str).to_list()\n",
        "\n",
        "y_single_train = df_train_single[\"complexity\"].astype(np.float32).to_numpy()\n",
        "y_single_test = df_test_single[\"complexity\"].astype(np.float32).to_numpy()\n",
        "\n",
        "sent_train_single_raw = df_train_single[\"sentence\"].to_list()\n",
        "sent_test_single_raw = df_test_single[\"sentence\"].to_list()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_s4hM7lDVWs"
      },
      "source": [
        "EMBEDDING_DIM = 50\n",
        "\n",
        "def get_embeddings():\n",
        "  embedding_index = {}\n",
        "  with open('glove.6B.{}d.txt'.format(EMBEDDING_DIM), 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      token = values[0]\n",
        "      embedding_index[token] = np.asarray(values[1:], dtype='float32')\n",
        "  return embedding_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aNXCbgrDK5O",
        "outputId": "f8e5a44c-d01f-4cb3-8b67-cde61c74f60f"
      },
      "source": [
        "embedding_index = get_embeddings()\n",
        "print('Token count in embeddings: {}'.format(len(embedding_index)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token count in embeddings: 400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QASfak9DbEY"
      },
      "source": [
        "HIDDEN_DIM = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhUBobPJDbtX"
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "  seq = seq.split()\n",
        "  idxs = [to_ix[w.lower()] if w.lower() in to_ix else len(to_ix) for w in seq]\n",
        "  idxs = torch.tensor(idxs)\n",
        "  idxs = nn.functional.one_hot(idxs, num_classes=len(to_ix))\n",
        "  idxs = torch.tensor(idxs, dtype=torch.float32)\n",
        "  return idxs\n",
        "\n",
        "\n",
        "def map_token_to_idx():\n",
        "  word_to_ix = {}\n",
        "  for sent in sent_train_single_raw:\n",
        "    sent = sent.split()\n",
        "    for word in sent:\n",
        "      word = word.lower()\n",
        "      if word not in word_to_ix:\n",
        "        word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "  \n",
        "  \n",
        "  return word_to_ix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuadQ1w1DbzI",
        "outputId": "f6f3580b-0996-4ba1-97b2-013be4a2864c"
      },
      "source": [
        "word_to_ix = map_token_to_idx()\n",
        "print('SWE vocab size: {}'.format(len(word_to_ix), ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SWE vocab size: 24350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8AIB1zcDjUW"
      },
      "source": [
        "class biLSTM(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size):\n",
        "    super(biLSTM, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "    self.hidden2tag = nn.Linear(2 * hidden_dim, output_size)\n",
        "\n",
        "  def prepare_embedding(self, sentence):\n",
        "    embeddings = []\n",
        "    for word in sentence:\n",
        "      word = word.lower()\n",
        "      if word in embedding_index:\n",
        "        embeddings.extend(embedding_index[word])\n",
        "      else:\n",
        "        embeddings.extend(np.random.random(EMBEDDING_DIM).tolist())\n",
        "    embeddings = torch.tensor(embeddings, dtype=torch.float32, device=device)\n",
        "    return embeddings\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    sentence = sentence.split()\n",
        "    embeds = self.prepare_embedding(sentence)\n",
        "    lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "    tag_scores = F.softmax(tag_space, dim=1)\n",
        "    return tag_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhvDrh-DHB9p"
      },
      "source": [
        "model = biLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(word_to_ix))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuwKeqSmHE3M"
      },
      "source": [
        "USE_PRETRAINED_SINGLE_WORD_TARGET_MODEL = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7n-e_LSHIT9",
        "outputId": "6aa3bb1f-a5b7-45c3-edef-050a4564797d"
      },
      "source": [
        "if USE_PRETRAINED_SINGLE_WORD_TARGET_MODEL:\n",
        "  print('Using pre-trained biLSTM on single target expressions')\n",
        "  model = torch.load(path_biLSTM_single)\n",
        "  model.eval()\n",
        "else:\n",
        "  print('Training biLSTM on single target expressions')\n",
        "  # Train the model for 10 epochs\n",
        "  model = biLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(word_to_ix))\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "  for epoch in range(1):\n",
        "    loss_sum = 0\n",
        "    for sentence in sent_train_single_raw:\n",
        "      model.zero_grad()\n",
        "      targets = prepare_sequence(sentence, word_to_ix)\n",
        "      tag_scores = model(sentence)\n",
        "      loss = loss_function(tag_scores, targets)\n",
        "      loss_sum += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print('Epoch: {} Loss: {}'.format(epoch, loss_sum.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training biLSTM on single target expressions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 0.2653784155845642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCYIKULVHcsW"
      },
      "source": [
        "def prepare_features_single_word(tokens, sentences):\n",
        "  features = []\n",
        "  for idx, word in enumerate(tokens):\n",
        "    word = word.lower()\n",
        "    feature = []\n",
        "\n",
        "    # Word length\n",
        "    feature.append(len(word))\n",
        "    doc = nlp(word)\n",
        "\n",
        "    # Syllable count and word frequency in the corpus\n",
        "    # Spacy tokenizes the input sentence\n",
        "    # In this case we would have only one token, the target word\n",
        "    for token in doc:\n",
        "      feature.append(token._.syllables_count)\n",
        "      feature.append(word_frequency(word, 'en'))\n",
        "\n",
        "    # Probability of target word `word` in the sentence estimated from by `model`\n",
        "    if word in word_to_ix:\n",
        "      # Output scores for each of the word in the sentence\n",
        "      out = model(sentences[idx])\n",
        "      pos = -1\n",
        "      for itr, token in enumerate(sentences[idx].split()):\n",
        "        if token.lower() == word:\n",
        "          pos = itr\n",
        "          break\n",
        "      id_pos = word_to_ix[word] # word to id mapping\n",
        "      feature.append(float(out[pos][id_pos]))\n",
        "    else:\n",
        "      # `word` not in vocabulary, so cannot predict probability in context\n",
        "      feature.append(0.0)\n",
        "\n",
        "    # GloVE embedding for the `word`\n",
        "    if word in embedding_index:\n",
        "      feature.extend(embedding_index[word].tolist())\n",
        "    else:\n",
        "      # `word` not in the GloVE corpus, take a random embedding\n",
        "      feature.extend(np.random.random(EMBEDDING_DIM).tolist())\n",
        "    features.append(feature)\n",
        "\n",
        "    if (idx + 1) % 500 == 0:\n",
        "      print('Prepared features for {} single target word sentences'.format(idx + 1))\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F09gSURIHjaF",
        "outputId": "d5554a02-d19b-4456-c9ff-b8be9bc61f7b"
      },
      "source": [
        "print('+++ Generating Train features for Single word expressions +++')\n",
        "features_train_single = prepare_features_single_word(single_tokens_train_raw, sent_train_single_raw)\n",
        "print('+++ Generating Test features for Single word expressions +++')\n",
        "features_test_single = prepare_features_single_word(single_tokens_test_raw, sent_test_single_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++ Generating Train features for Single word expressions +++\n",
            "Prepared features for 500 single target word sentences\n",
            "Prepared features for 1000 single target word sentences\n",
            "Prepared features for 1500 single target word sentences\n",
            "Prepared features for 2000 single target word sentences\n",
            "Prepared features for 2500 single target word sentences\n",
            "Prepared features for 3000 single target word sentences\n",
            "Prepared features for 3500 single target word sentences\n",
            "Prepared features for 4000 single target word sentences\n",
            "Prepared features for 4500 single target word sentences\n",
            "Prepared features for 5000 single target word sentences\n",
            "Prepared features for 5500 single target word sentences\n",
            "Prepared features for 6000 single target word sentences\n",
            "Prepared features for 6500 single target word sentences\n",
            "Prepared features for 7000 single target word sentences\n",
            "Prepared features for 7500 single target word sentences\n",
            "+++ Generating Test features for Single word expressions +++\n",
            "Prepared features for 500 single target word sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pSjj7_iIBv4"
      },
      "source": [
        "# Convert all features to torch.tensor to enable use in PyTorch models\n",
        "X_train_single_tensor = torch.tensor(features_train_single, dtype=torch.float32, device=device)\n",
        "X_test_single_tensor = torch.tensor(features_test_single, dtype=torch.float32, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJiWNy4eIByV"
      },
      "source": [
        "# Reshape all output complexity scores to single dimension vectors\n",
        "y_single_train = y_single_train.reshape(y_single_train.shape[0], -1)\n",
        "y_single_test = y_single_test.reshape(y_single_test.shape[0], -1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtEEvlspIB0p"
      },
      "source": [
        "# Convert all target outputs to torch.tensor to enable use in PyTorch models\n",
        "Y_train_single_tensor = torch.tensor(y_single_train, dtype=torch.float32, device=device)\n",
        "Y_test_single_tensor = torch.tensor(y_single_test, dtype=torch.float32, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TR7LSa-ILHb",
        "outputId": "e23021b4-1ffb-4d26-ef16-3c68484e2d98"
      },
      "source": [
        "# Ensure each sample from test and train for single word expression is taken\n",
        "print(X_train_single_tensor.shape)\n",
        "print(X_test_single_tensor.shape)\n",
        "print(Y_train_single_tensor.shape)\n",
        "print(Y_test_single_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7662, 54])\n",
            "torch.Size([917, 54])\n",
            "torch.Size([7662, 1])\n",
            "torch.Size([917, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-5OihdlIQSa"
      },
      "source": [
        "def convert_tensor_to_np(y):\n",
        "  if device == torch.device(\"cuda\"):\n",
        "    y = y.cpu()\n",
        "  y = y.detach().numpy()\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0xseowDISgn"
      },
      "source": [
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu7VFKF2IVdF"
      },
      "source": [
        "# Evaluate the metrics upon which the model would be evaluated\n",
        "def evaluate_metrics(labels, predicted):\n",
        "  vx, vy = [], []\n",
        "  if torch.is_tensor(labels):\n",
        "    vx = labels.clone()\n",
        "    vx = convert_tensor_to_np(vx)\n",
        "  else:\n",
        "    vx = deepcopy(labels)\n",
        "  if torch.is_tensor(predicted):\n",
        "    vy = predicted.clone()\n",
        "    vy = convert_tensor_to_np(vy)\n",
        "  else:\n",
        "    vy = deepcopy(predicted)\n",
        "\n",
        "  pearsonR = np.corrcoef(vx.T, vy.T)[0, 1]\n",
        "  spearmanRho = stats.spearmanr(vx, vy)\n",
        "  MSE = np.mean((vx - vy) ** 2)\n",
        "  MAE = np.mean(np.absolute(vx - vy))\n",
        "  RSquared = pearsonR ** 2\n",
        "\n",
        "  print(\"Peason's R: {}\".format(pearsonR))\n",
        "  print(\"Spearman's rho: {}\".format(spearmanRho))\n",
        "  print(\"R Squared: {}\".format(RSquared))\n",
        "  print(\"MSE: {}\".format(MSE))\n",
        "  print(\"MAE: {}\".format(MAE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df92J1gTSg9A"
      },
      "source": [
        "X_train_single_np = np.array(features_train_single)\n",
        "X_test_single_np = np.array(features_test_single)\n",
        "Y_train_single_np = np.array(y_single_train.reshape(y_single_train.shape[0], -1))\n",
        "Y_test_single_np = np.array(y_single_test.reshape(y_single_test.shape[0], -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2lrKGFzSmjs",
        "outputId": "7407dd83-60a7-43da-8f03-87ab4de54939"
      },
      "source": [
        "print(X_train_single_np.shape)\n",
        "print(X_test_single_np.shape)\n",
        "print(Y_train_single_np.shape)\n",
        "print(Y_test_single_np.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7662, 54)\n",
            "(917, 54)\n",
            "(7662, 1)\n",
            "(917, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CvstUkWUM2e"
      },
      "source": [
        "#LR MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNWFTlsfSwEC"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpQtfp9NSzF_"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_YKjNSrSzIK"
      },
      "source": [
        "def evaluateLinearRegression(X_train, Y_train, X_test, Y_test):\n",
        "  reg = make_pipeline(StandardScaler(), LinearRegression())\n",
        "  reg.fit(X_train, Y_train)\n",
        "  out = reg.predict(X_test)\n",
        "  out = out.reshape((out.shape[0], 1))\n",
        "  evaluate_metrics(out, Y_test)\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bsyra1IS8Nd",
        "outputId": "4284be4c-1fe7-4eb1-87cc-25757d192451"
      },
      "source": [
        "print('Linear Regression for Single word expressions')\n",
        "out_LR = evaluateLinearRegression(X_train_single_np, Y_train_single_np, X_test_single_np, Y_test_single_np)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression for Single word expressions\n",
            "Peason's R: 0.7131966978387039\n",
            "Spearman's rho: SpearmanrResult(correlation=0.6922427095275896, pvalue=9.064053173897807e-132)\n",
            "R Squared: 0.5086495298080315\n",
            "MSE: 0.00795479190989954\n",
            "MAE: 0.06836953159067313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaywUecIUER0"
      },
      "source": [
        "#SVR MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n4wMStMSzJ6"
      },
      "source": [
        "from sklearn.svm import SVR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZM2J5kWTJPp"
      },
      "source": [
        "def evaluateSVR(X_train, Y_train, X_test, Y_test):\n",
        "  svr = make_pipeline(StandardScaler(), SVR(C=0.05, epsilon=0.01))\n",
        "  svr.fit(X_train, Y_train.reshape(-1))\n",
        "  out = svr.predict(X_test)\n",
        "  out = out.reshape((out.shape[0], 1))\n",
        "  evaluate_metrics(out, Y_test)\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFeQ8CD5TKEY",
        "outputId": "bea961cb-f471-4a97-b243-622b22d02517"
      },
      "source": [
        "print('SVR for Single word expressions')\n",
        "out_svr = evaluateSVR(X_train_single_np, Y_train_single_np, X_test_single_np, Y_test_single_np)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVR for Single word expressions\n",
            "Peason's R: 0.7303058792979216\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7055868915357708, pvalue=5.038054858837159e-139)\n",
            "R Squared: 0.5333466773371104\n",
            "MSE: 0.007586501609013984\n",
            "MAE: 0.06758995521244428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bpUvgqUTUoU"
      },
      "source": [
        "single_ids = df_test_single[\"id\"].astype(str).to_list()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwRivT1MTX9t",
        "outputId": "f28f2e34-7123-4972-9a91-7ce0cb7581c6"
      },
      "source": [
        "out_ensemble = []\n",
        "\n",
        "for idx in range(len(out_LR)):\n",
        "  score = 0\n",
        "  score += float(out_LR[idx])\n",
        "  score += float(out_svr[idx])\n",
        "  if idx == 0:\n",
        "    print(float(out_LR[idx]), float(out_svr[idx]), score / 2)\n",
        "  score /= 2\n",
        "  out_ensemble.append(score)\n",
        "out_ensemble = np.array(out_ensemble)\n",
        "out_ensemble = out_ensemble.reshape((out_ensemble.shape[0], 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11769032925997991 0.11241302335320352 0.11505167630659172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAf8nxhETbPP",
        "outputId": "ea294f1a-5b9f-49fb-a97d-8ad55ef57de1"
      },
      "source": [
        "# Score from the Overall Model for Single Word Expressions\n",
        "evaluate_metrics(out_ensemble, Y_test_single_np)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peason's R: 0.7325897290253571\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7082096676011037, pvalue=1.6904547334692234e-140)\n",
            "R Squared: 0.5366877110734461\n",
            "MSE: 0.007527301605110034\n",
            "MAE: 0.06697485526324143\n"
          ]
        }
      ]
    }
  ]
}